{
	"metadata": {
		"colab": {
			"gpuType": "L4",
			"provenance": []
		},
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# Import Libraries and Start Spark Session",
			"metadata": {
				"id": "42qCZvAxbivP"
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import create_map, lit\nfrom pyspark.sql.window import Window\nfrom itertools import chain",
			"metadata": {
				"trusted": true,
				"tags": [],
				"id": "aTp9DR3wba0b"
			},
			"execution_count": 47,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Initialize Glue and Spark\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 48,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# 1. Get Combined Dataset of Carparks Availability",
			"metadata": {
				"id": "KgeqHmjoeNID"
			}
		},
		{
			"cell_type": "code",
			"source": "# load lta carparks availability dataset\nfile_path = \"s3://athena-results-090325/Unsaved/2025/03/30/1eedf5d9-0ed0-493d-8991-d98b9b26727a.csv\"\ndf1 = spark.read.option(\"header\", \"true\").csv(file_path)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"id": "JZHsXqRdwRRH"
			},
			"execution_count": 49,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#filter lot type for cars only\ndf1 = df1.filter(df1[\"lot_type\"] == \"C\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"id": "JDwUaCa5dl9k"
			},
			"execution_count": 50,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#fill NA for total_lots and available_lots\ndf1 = df1.fillna({\"total_lots\": 0})\ndf1 = df1.fillna({\"available_lots\": 0})",
			"metadata": {
				"trusted": true,
				"tags": [],
				"id": "hZBxS8o2dtD3"
			},
			"execution_count": 51,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#change datatype for available lots and total lots to integer\ndf1 = df1.withColumn(\"available_lots\", F.col(\"available_lots\").cast(\"integer\"))\ndf1 = df1.withColumn(\"total_lots\", F.col(\"total_lots\").cast(\"integer\"))",
			"metadata": {
				"trusted": true,
				"tags": [],
				"id": "gc6q8xguXX7e"
			},
			"execution_count": 52,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df1_agg = df1.groupBy(\"carpark_id\", \"development\", \"location\", \"agency\", \"source\") \\\n    .agg(\n        F.max(\"total_lots\").alias(\"total_lots\"),\n        F.max(\"available_lots\").alias(\"available_lots\")\n    )",
			"metadata": {
				"trusted": true,
				"tags": [],
				"id": "bXort9sufn2b"
			},
			"execution_count": 53,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "Check Total Lots Values",
			"metadata": {
				"id": "Cp2aoBP_gxUo"
			}
		},
		{
			"cell_type": "code",
			"source": "#if total_lots = 0, update with max available_lots value\ndf1_agg = df1_agg.withColumn(\"total_lots\", F.when(df1_agg[\"total_lots\"] == 0, df1_agg[\"available_lots\"]).otherwise(df1_agg[\"total_lots\"]))",
			"metadata": {
				"trusted": true,
				"tags": [],
				"id": "u1gLhTYThT2y"
			},
			"execution_count": 54,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#count carparks with total_lots = 0\nprint(df1_agg.filter(df1_agg[\"total_lots\"] == 0).count())",
			"metadata": {
				"trusted": true,
				"tags": [],
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "rNG_n4SPhZEE",
				"outputId": "09700180-91f4-42cc-c864-bfa76794abfb"
			},
			"execution_count": 55,
			"outputs": [
				{
					"name": "stdout",
					"text": "7\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Check if any carparks > 5000 total lots\ndf1_agg.filter(df1_agg[\"total_lots\"] > 5000).show()",
			"metadata": {
				"trusted": true,
				"tags": [],
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "D0bQRDTVlzhY",
				"outputId": "d2a6ebe1-f74f-40f1-a3a1-10dd633d1a47"
			},
			"execution_count": 56,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+--------------------+--------------------+------+------+----------+--------------+\n|carpark_id|         development|            location|agency|source|total_lots|available_lots|\n+----------+--------------------+--------------------+------+------+----------+--------------+\n|       BM4|                null|                null|  null|   hdb|      9999|          9913|\n|       BM4|BLK 35A JALAN RUM...|1.287667284247440...|   HDB|   lta|      9907|          9907|\n+----------+--------------------+--------------------+------+------+----------+--------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#update BM4 total lots to 150\ndf1_agg = df1_agg.withColumn(\"total_lots\", F.when(df1_agg[\"carpark_id\"] == \"BM4\", 150).otherwise(df1_agg[\"total_lots\"]))",
			"metadata": {
				"trusted": true,
				"tags": [],
				"id": "y60gjGWWptHy"
			},
			"execution_count": 57,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# 2. Combine Carpark Locations for HDB and LTA Dataset - Lat / Long and XY Coordinates",
			"metadata": {
				"id": "ncdAU9msqwH6"
			}
		},
		{
			"cell_type": "code",
			"source": "#split the location into lat and long\ndf1_agg = df1_agg.withColumn(\"location_split\", F.split(df1_agg[\"location\"], \" \")) \\\n                 .withColumn(\"latitude\", F.col(\"location_split\").getItem(0)) \\\n                 .withColumn(\"longitude\", F.col(\"location_split\").getItem(1)) \\\n                 .drop(\"location_split\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"id": "mnnlXfyhqsUu"
			},
			"execution_count": 58,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#rearrange columns\ndf1_agg = df1_agg[[\"carpark_id\", \"development\", \"latitude\", \"longitude\", \"total_lots\", \"agency\", \"source\"]]",
			"metadata": {
				"trusted": true,
				"tags": [],
				"id": "j23Rcr7zrTIn"
			},
			"execution_count": 59,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#rename columns\ndf1_agg = df1_agg.withColumnRenamed(\"development\", \"address\") \\\n                 .withColumnRenamed(\"source\", \"dataset\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"id": "xpVwTYDPrY2y"
			},
			"execution_count": 60,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "Get HDB Carparks Dataset for Long & Lat of Carparks from HDB Dataset",
			"metadata": {
				"id": "WRYCBaUcsLjQ"
			}
		},
		{
			"cell_type": "code",
			"source": "# load HDB carparks info dataset\nfile_path = \"s3://hdb-carpark-info/HDBCarparkInformation.csv\"\ndf2 = spark.read.option(\"header\", \"true\").csv(file_path)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"id": "-Imihn7asOc2"
			},
			"execution_count": 68,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#check df2 for duplicate car_park_no\nduplicate_car_park_no = df2.groupBy(\"car_park_no\").count().filter(F.col(\"count\") > 1)\nduplicate_car_park_no.show(5)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"id": "xIxD8FIjsubf"
			},
			"execution_count": 67,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+-----+\n|car_park_no|count|\n+-----------+-----+\n+-----------+-----+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#select columns from df2\ndf2 = df2[[\"car_park_no\", \"address\", \"x_coord\", \"y_coord\"]]\n\n#rename columns\ndf2 = df2.withColumnRenamed(\"car_park_no\", \"carpark_id\") \\\n                  .withColumnRenamed(\"address\", \"address_hdb\") \\\n                 .withColumnRenamed(\"x_coord\", \"x_coordinate\") \\\n                 .withColumnRenamed(\"y_coord\", \"y_coordinate\")",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#merge df1_agg and df2\ndfs_merge = df1_agg.join(df2, on=\"carpark_id\", how=\"left\")",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#if address is null, update from address_hdb\ndfs_merge = dfs_merge.withColumn(\"address\", F.when(F.col(\"address\").isNull(), F.col(\"address_hdb\")).otherwise(F.col(\"address\")))",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#rearrange columns\ndfs_merge = dfs_merge[[\"carpark_id\", \"address\", \"latitude\", \"longitude\", \"x_coordinate\", \"y_coordinate\", \"total_lots\", \"agency\", \"dataset\"]]",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#if duplicate carpark_id, keep those from dataset = hdb\ndfs_merge = dfs_merge.withColumn(\n    \"priority\",\n    F.when((F.col(\"agency\") == \"HDB\") & (F.col(\"dataset\") == \"hdb\"), 1).otherwise(2)\n)\n\n# Step 2: Apply row_number window over carpark_id\nwindow = Window.partitionBy(\"carpark_id\").orderBy(\"priority\")\n\n# Step 3: Keep only the top-priority row per carpark_id\ndfs_merge = dfs_merge.withColumn(\"row_num\", F.row_number().over(window)) \\\n                     .filter(F.col(\"row_num\") == 1) \\\n                     .drop(\"priority\", \"row_num\")",
			"metadata": {
				"id": "HUroiixKw9_o"
			},
			"execution_count": 30,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "# 3. Update Lat / Long, X_Coord / Y_Coord",
			"metadata": {
				"id": "fzFn8ysHoGY3"
			}
		},
		{
			"cell_type": "code",
			"source": "#import libraries\nimport pyproj\nimport geopandas as gpd\nfrom shapely.geometry import Point\nfrom pyproj import CRS, Transformer\n\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, when, udf, struct\nfrom pyspark.sql.types import StructType, StructField, DoubleType",
			"metadata": {
				"trusted": true,
				"tags": [],
				"id": "F5ENwEWW5u1H"
			},
			"execution_count": 90,
			"outputs": [
				{
					"name": "stdout",
					"text": "ModuleNotFoundError: No module named 'pyproj'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "if x_coordinate and y_coordinate are NULL, update by mapping the same location from latitude and longitude. Format accepted by OneMap API",
			"metadata": {
				"id": "0i9B42hW1m_T"
			}
		},
		{
			"cell_type": "code",
			"source": "# Cast lat/lon before applying UDF\ndfs_merge = dfs_merge.withColumn(\"latitude\", F.col(\"latitude\").cast(\"double\"))\ndfs_merge = dfs_merge.withColumn(\"longitude\", F.col(\"longitude\").cast(\"double\"))\n\n# Define UDF function\ndef transform_coords(lat, lon):\n    transformer = pyproj.Transformer.from_crs(\"EPSG:4326\", \"EPSG:3414\", always_xy=True)\n    try:\n        x, y = transformer.transform(float(lon), float(lat))\n        return x, y\n    except:\n        return None, None\n\n# Register UDF\nschema = StructType([\n    StructField(\"x\", DoubleType(), True),\n    StructField(\"y\", DoubleType(), True)\n])\ntransform_coords_udf = F.udf(transform_coords, schema)\n\n# Apply UDF to fill in missing x/y\ndfs_merge = dfs_merge.withColumn(\n    \"x_coordinate\",\n    F.when(\n        F.col(\"x_coordinate\").isNull(),\n        transform_coords_udf(\"latitude\", \"longitude\").getItem(\"x\")\n    ).otherwise(F.col(\"x_coordinate\"))\n)\n\ndfs_merge = dfs_merge.withColumn(\n    \"y_coordinate\",\n    F.when(\n        F.col(\"y_coordinate\").isNull(),\n        transform_coords_udf(\"latitude\", \"longitude\").getItem(\"y\")\n    ).otherwise(F.col(\"y_coordinate\"))\n)",
			"metadata": {
				"id": "Hy4nDkMSw-sy"
			},
			"execution_count": 32,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "If latitude and longitude are NULL, update by mapping the same location from x_coordinate and y_coordinate . Format accepted by OneMap API",
			"metadata": {
				"id": "rgPBsTps1rYq"
			}
		},
		{
			"cell_type": "code",
			"source": "# Define the projection systems using pyproj\nwgs84 = pyproj.CRS('EPSG:4326')  # WGS84\nsvy21 = pyproj.CRS('EPSG:3414')  # SVY21\ntransformer = pyproj.Transformer.from_crs(svy21, wgs84, always_xy=True)\n\n# UDF to convert SVY21 to WGS84 (convert x and y to longitude and latitude)\ndef svy21_to_wgs84(x, y):\n    if x is None or y is None:\n        return None, None\n    # Ensure x and y are scalar values\n    lon, lat = transformer.transform(float(x), float(y))  # Convert to float to ensure scalars\n    # Return as a Row (which is equivalent to a struct)\n    return Row(longitude=lon, latitude=lat)\n\n# Register the UDF with a StructType return type (longitude, latitude)\nsvy21_to_wgs84_udf = F.udf(svy21_to_wgs84, StructType([\n    StructField(\"longitude\", DoubleType()),\n    StructField(\"latitude\", DoubleType())\n]))\n\n# Update missing latitude and longitude by applying the reverse transformation on x and y coordinates\ndfs_merge = dfs_merge.withColumn(\n    'longitude',\n    F.when(F.col('longitude').isNull(), svy21_to_wgs84_udf(F.col('x_coordinate'), F.col('y_coordinate'))['longitude'])\n    .otherwise(F.col('longitude'))\n)\n\ndfs_merge = dfs_merge.withColumn(\n    'latitude',\n    F.when(F.col('latitude').isNull(), svy21_to_wgs84_udf(F.col('x_coordinate'), F.col('y_coordinate'))['latitude'])\n    .otherwise(F.col('latitude'))\n)\n",
			"metadata": {
				"id": "Yi0dy_hvMCzl"
			},
			"execution_count": 33,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "# Update Values for Lat/Long and X/Y Coordinate",
			"metadata": {
				"id": "5vxxom4Fa71o"
			}
		},
		{
			"cell_type": "code",
			"source": "# Create a list of carpark_ids to update\ncarpark_ids_to_update = [\"C32\", \"CR6\", \"PL90\", \"TH3L\", \"THPP\", \"W56L\"]\n\n# Create a list of corresponding new addresses\nnew_addresses = [\"BLK 514-519 WEST COAST ROAD\", \"BLK 10 NORTH BRIDGE ROAD\", \"BLK 445 & 446 PUNGGOL WAY\", \"BLK 123A -123C,125A-125B,126A -126C TENGAH DRIVE\", \"BLK 128A & 128B PLANTATION CRESCENT\", \"891C WOODLANDS DRIVE 50\"]\n\n# Create a dictionary for efficient lookup\naddress_updates = dict(zip(carpark_ids_to_update, new_addresses))",
			"metadata": {
				"id": "nzy7sprNa6x3"
			},
			"execution_count": 34,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#update address column for selected carpark_id\nmapping_expr = create_map([lit(x) for x in chain(*address_updates.items())])\n\ndfs_merge = dfs_merge.withColumn(\n    \"address\",\n    when(col(\"carpark_id\").isin(carpark_ids_to_update), mapping_expr.getItem(col(\"carpark_id\"))).otherwise(col(\"address\"))\n)",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "GEeGuhYabOfE",
				"outputId": "b0bf09d8-1a9f-48d5-e813-c7abb64b4cd3"
			},
			"execution_count": 35,
			"outputs": [
				{
					"output_type": "stream",
					"name": "stderr",
					"text": "/usr/local/lib/python3.11/dist-packages/pyspark/sql/column.py:460: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n\n  warnings.warn(\n"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#drop vull values for null address, lat and long\ndfs_merge = dfs_merge.filter(F.col(\"address\").isNotNull() & F.col(\"latitude\").isNotNull() & F.col(\"longitude\").isNotNull())",
			"metadata": {
				"id": "h4C0Tm4wafRH"
			},
			"execution_count": 36,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "dfs_merge = dfs_merge.withColumn(\"x_coordinate\", col(\"x_coordinate\").cast(\"double\")) \\\n       .withColumn(\"y_coordinate\", col(\"y_coordinate\").cast(\"double\"))",
			"metadata": {
				"id": "bapmEHOyoWl1"
			},
			"execution_count": 37,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "# Get Postal Code Based on OneMap API",
			"metadata": {
				"id": "O_OicIqwaGZi"
			}
		},
		{
			"cell_type": "code",
			"source": "import json\nimport requests\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType",
			"metadata": {
				"id": "QRFqlVFHfuC6"
			},
			"execution_count": 38,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#add postal code column after address\ndfs_merge = dfs_merge.withColumn(\"postal_code\", lit(None).cast(\"string\"))",
			"metadata": {
				"id": "ibiaaKOFgLPh"
			},
			"execution_count": 39,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Authenticate & Get API Token\ndef get_token():\n    url = \"https://www.onemap.gov.sg/api/auth/post/getToken\"\n    data = {\n        \"email\": \"e0997996@u.nus.edu\",\n        \"password\": \"Munchkin1993!\"\n    }\n    response = requests.post(url, json=data)\n    return response.json().get(\"access_token\")\n\ntoken = get_token()\nprint(\"Access Token:\", token)",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "LxtKvu-6fwmF",
				"outputId": "ca70ee61-a85d-417e-e7bf-fa18f291ee7c"
			},
			"execution_count": 40,
			"outputs": [
				{
					"output_type": "stream",
					"name": "stdout",
					"text": "Access Token: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiIyOTU1NmZjN2I2ZWMxYWI1NGU3MWEzNGNlOWU0ZGRlNCIsImlzcyI6Imh0dHA6Ly9pbnRlcm5hbC1hbGItb20tcHJkZXppdC1pdC1uZXctMTYzMzc5OTU0Mi5hcC1zb3V0aGVhc3QtMS5lbGIuYW1hem9uYXdzLmNvbS9hcGkvdjIvdXNlci9zZXNzaW9uIiwiaWF0IjoxNzQ0MjA4OTkzLCJleHAiOjE3NDQ0NjgxOTMsIm5iZiI6MTc0NDIwODk5MywianRpIjoiaDlkaWd6eHRQMjBGc2YzVyIsInVzZXJfaWQiOjYxMzMsImZvcmV2ZXIiOmZhbHNlfQ.gt-leVRPHmSsdGUHHV5xNn2yIx2Jtc0NhZ3nqfYc4_A\n"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#update postal_code using address\ndef fetch_postal(address):\n    try:\n        if not address:\n            return None\n\n        url = (\n            f\"https://www.onemap.gov.sg/api/common/elastic/search\"\n            f\"?searchVal={address}&returnGeom=Y&getAddrDetails=Y&pageNum=1\"\n        )\n\n        headers = {\"Authorization\": f\"Bearer {token}\"}\n        response = requests.get(url, headers=headers, timeout=5)\n\n        if response.status_code == 200:\n            result = response.json()\n            results = result.get(\"results\", [])\n\n            # Return the first POSTAL that is not \"NIL\" or None or empty string\n            for item in results:\n                postal = item.get(\"POSTAL\")\n                if postal and postal.strip().upper() != \"NIL\":\n                    return postal\n\n        else:\n            print(f\"Request failed with status code {response.status_code}\")\n\n    except Exception as e:\n        print(f\"Error fetching postal for address '{address}': {e}\")\n    return None\n\n# Register the UDF\nfetch_postal_udf = udf(fetch_postal, StringType())",
			"metadata": {
				"id": "jMitWO0Qg9TL"
			},
			"execution_count": 41,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#update postal_code column\ndf_with_postal = dfs_merge.withColumn(\"postal_code\", fetch_postal_udf(dfs_merge.address))",
			"metadata": {
				"id": "kYwbtzfafpFD"
			},
			"execution_count": 42,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "# Assign Area Based on OneMap API",
			"metadata": {
				"id": "x1r0zOeygJ0a"
			}
		},
		{
			"cell_type": "code",
			"source": "import json\nimport requests\nfrom pyspark.sql.functions import explode, udf, col\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, MapType\nfrom shapely.geometry import shape, MultiPolygon, Point",
			"metadata": {
				"id": "_atT_GRZoQ9n"
			},
			"execution_count": 44,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "Get the Planning Areas and Corresponding Longitude and Latitude",
			"metadata": {
				"id": "83kombcIbyUW"
			}
		},
		{
			"cell_type": "code",
			"source": "# Authenticate & Get API Token\ndef get_token():\n    url = \"https://www.onemap.gov.sg/api/auth/post/getToken\"\n    data = {\n        \"email\": \"e0997996@u.nus.edu\",\n        \"password\": \"Munchkin1993!\"\n    }\n    response = requests.post(url, json=data)\n    return response.json().get(\"access_token\")\n\ntoken = get_token()\nprint(\"Access Token:\", token)",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "wd2BJyDpmFWL",
				"outputId": "a250ef5f-2b27-4bb0-b7c5-e981ea1672f4"
			},
			"execution_count": 45,
			"outputs": [
				{
					"output_type": "stream",
					"name": "stdout",
					"text": "Access Token: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiIyOTU1NmZjN2I2ZWMxYWI1NGU3MWEzNGNlOWU0ZGRlNCIsImlzcyI6Imh0dHA6Ly9pbnRlcm5hbC1hbGItb20tcHJkZXppdC1pdC1uZXctMTYzMzc5OTU0Mi5hcC1zb3V0aGVhc3QtMS5lbGIuYW1hem9uYXdzLmNvbS9hcGkvdjIvdXNlci9zZXNzaW9uIiwiaWF0IjoxNzQ0MjA4OTkzLCJleHAiOjE3NDQ0NjgxOTMsIm5iZiI6MTc0NDIwODk5MywianRpIjoiaDlkaWd6eHRQMjBGc2YzVyIsInVzZXJfaWQiOjYxMzMsImZvcmV2ZXIiOmZhbHNlfQ.gt-leVRPHmSsdGUHHV5xNn2yIx2Jtc0NhZ3nqfYc4_A\n"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Fetch Planning Area GeoJSON Data\nurl = \"https://www.onemap.gov.sg/api/public/popapi/getAllPlanningarea?year=2019\"\nheaders = {\"Authorization\": token}\nresponse = requests.get(url, headers=headers)\ndata = response.json()\n\n#Normalize and parse geojson\nsearch_results = data[\"SearchResults\"]\ndf = spark.createDataFrame(search_results)\n\ndef get_multipolygon_from_geojson(geojson_str):\n    try:\n        geojson = json.loads(geojson_str)  # Parse string into dict\n        geom = shape(geojson)\n        if geom.geom_type == \"Polygon\":\n            return MultiPolygon([geom])\n        elif geom.geom_type == \"MultiPolygon\":\n            return geom\n    except Exception as e:\n        print(\"Error parsing geojson:\", e)\n    return None\n\n#Create list of (planning_area_name, geometry) tuples\nplanning_areas = []\nfor row in df.collect():\n    geom = get_multipolygon_from_geojson(row[\"geojson\"])\n    if geom:\n        planning_areas.append((row[\"pln_area_n\"], geom))\n\n# Broadcast the list to Spark\nbroadcast_areas = spark.sparkContext.broadcast(planning_areas)\n\n#Define a Spark UDF that finds which planning area a lat/lon point falls in\ndef find_planning_area(lat, lon):\n    try:\n        point = Point(lon, lat)\n        for name, poly in broadcast_areas.value:\n            if poly.contains(point):\n                return name\n    except:\n        return None\n    return None\n\nfind_planning_area_udf = udf(find_planning_area, StringType())\n\n#Apply the UDF and it contains 'latitude' and 'longitude' columns\ncp_info_area = df_with_postal.withColumn(\n    \"planning_area\",\n    find_planning_area_udf(\"latitude\", \"longitude\")\n)",
			"metadata": {
				"id": "FYL35JywVuTm"
			},
			"execution_count": 46,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#rearrange columns\ncp_info_area = cp_info_area[[\"carpark_id\", \"planning_area\", \"address\", \"postal_code\",  \"latitude\", \"longitude\", \"x_coordinate\", \"y_coordinate\", \"total_lots\", \"agency\", \"dataset\"]]\n#rename column\ncp_info_area = cp_info_area.withColumnRenamed(\"planning_area\", \"area\")",
			"metadata": {
				"id": "mT_f-KpgHnFE"
			},
			"execution_count": 47,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#add update_datetime to current datetime in format dd/mm/yyyy hh:mm\ncp_info_area = cp_info_area.withColumn(\"update_datetime\", F.current_timestamp())",
			"metadata": {
				"id": "fk2VCIRBHz11"
			},
			"execution_count": 48,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#add update_datetime to current datetime in format dd/mm/yyyy hh:mm\ncp_info_area = cp_info_area.withColumn(\n    \"update_datetime\",\n    F.date_format(F.current_timestamp(), \"dd/MM/yyyy HH:mm:ss\")\n)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#change postal code type to string\ncp_info_area = cp_info_area.withColumn(\"postal_code\", col(\"postal_code\").cast(\"string\"))",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "# Output Final Table",
			"metadata": {
				"id": "DNFtXkf4GRr6"
			}
		},
		{
			"cell_type": "code",
			"source": "cp_info_area.write.mode(\"overwrite\").option(\"header\", True).csv(\"s3://carpark-information/carpark_information_full.csv\")",
			"metadata": {
				"id": "7yEvGkZuGYsQ"
			},
			"execution_count": null,
			"outputs": []
		}
	]
}